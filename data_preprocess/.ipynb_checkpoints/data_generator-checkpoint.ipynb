{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To generate 2D hdf5 patches: Siastole and Diastole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os, sys, shutil, time, re\n",
    "import scipy.ndimage as snd\n",
    "import h5py\n",
    "import SimpleITK as sitk\n",
    "import skimage.morphology as morph\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import animation, rc\n",
    "import cv2\n",
    "import time\n",
    "import cPickle as pickle\n",
    "# For ROI extraction\n",
    "import skimage.transform\n",
    "from scipy.fftpack import fftn, ifftn\n",
    "from skimage.feature import peak_local_max, canny\n",
    "from skimage.transform import hough_circle \n",
    "# Nifti processing\n",
    "import nibabel as nib\n",
    "from collections import OrderedDict\n",
    "# print sys.path\n",
    "# sys.path.append(\"..\") \n",
    "from IPython.display import HTML\n",
    "import errno\n",
    "# Heart Metrics: TODO\n",
    "%matplotlib inline\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fourier-Hough Transform Based ROI Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_roi(data4D, pixel_spacing, minradius_mm=15, maxradius_mm=45, kernel_width=5, \n",
    "                center_margin=8, num_peaks=10, num_circles=20, radstep=2):\n",
    "    \"\"\"\n",
    "    Returns center and radii of ROI region in (i,j) format\n",
    "    \"\"\"\n",
    "    # Data shape: \n",
    "    # radius of the smallest and largest circles in mm estimated from the train set\n",
    "    # convert to pixel counts\n",
    "\n",
    "    pixel_spacing_X, pixel_spacing_Y, _,_ = pixel_spacing\n",
    "    minradius = int(minradius_mm / pixel_spacing_X)\n",
    "    maxradius = int(maxradius_mm / pixel_spacing_Y)\n",
    "\n",
    "    ximagesize = data4D.shape[0]\n",
    "    yimagesize = data4D.shape[1]\n",
    "    zslices = data4D.shape[2]\n",
    "    tframes = data4D.shape[3]\n",
    "    xsurface = np.tile(range(ximagesize), (yimagesize, 1)).T\n",
    "    ysurface = np.tile(range(yimagesize), (ximagesize, 1))\n",
    "    lsurface = np.zeros((ximagesize, yimagesize))\n",
    "\n",
    "    allcenters = []\n",
    "    allaccums = []\n",
    "    allradii = []\n",
    "\n",
    "    for slice in range(zslices):\n",
    "        ff1 = fftn([data4D[:,:,slice, t] for t in range(tframes)])\n",
    "        fh = np.absolute(ifftn(ff1[1, :, :]))\n",
    "        fh[fh < 0.1 * np.max(fh)] = 0.0\n",
    "        image = 1. * fh / np.max(fh)\n",
    "#         plt.imshow(image, cmap = 'gray')\n",
    "#         plt.show()\n",
    "        # find hough circles and detect two radii\n",
    "        edges = canny(image, sigma=3)\n",
    "        # plt.imshow(edges, cmap = 'gray')\n",
    "        # plt.show()\n",
    "        hough_radii = np.arange(minradius, maxradius, radstep)\n",
    "        # print hough_radii\n",
    "        hough_res = hough_circle(edges, hough_radii)\n",
    "        # print hough_res.shape\n",
    "#         plt.imshow(hough_res[0,:,:], cmap = 'gray')\n",
    "#         plt.show()                                \n",
    "        if hough_res.any():\n",
    "            centers = []\n",
    "            accums = []\n",
    "            radii = []\n",
    "\n",
    "            for radius, h in zip(hough_radii, hough_res):\n",
    "                # For each radius, extract num_peaks circles\n",
    "                peaks = peak_local_max(h, num_peaks=num_peaks)\n",
    "                centers.extend(peaks)\n",
    "                accums.extend(h[peaks[:, 0], peaks[:, 1]])\n",
    "                radii.extend([radius] * num_peaks)\n",
    "  \n",
    "            # Keep the most prominent num_circles circles\n",
    "            sorted_circles_idxs = np.argsort(accums)[::-1][:num_circles]\n",
    "\n",
    "            for idx in sorted_circles_idxs:\n",
    "                center_x, center_y = centers[idx]\n",
    "                allcenters.append(centers[idx])\n",
    "                allradii.append(radii[idx])\n",
    "                allaccums.append(accums[idx])\n",
    "                brightness = accums[idx]\n",
    "                lsurface = lsurface + brightness * np.exp(\n",
    "                    -((xsurface - center_x) ** 2 + (ysurface - center_y) ** 2) / kernel_width ** 2)\n",
    "\n",
    "    lsurface = lsurface / lsurface.max()\n",
    "#     plt.imshow(lsurface, cmap = 'gray')\n",
    "#     plt.show() \n",
    "    # select most likely ROI center\n",
    "    roi_center = np.unravel_index(lsurface.argmax(), lsurface.shape)\n",
    "\n",
    "    # determine ROI radius\n",
    "    roi_x_radius = 0\n",
    "    roi_y_radius = 0\n",
    "    for idx in range(len(allcenters)):\n",
    "        xshift = np.abs(allcenters[idx][0] - roi_center[0])\n",
    "        yshift = np.abs(allcenters[idx][1] - roi_center[1])\n",
    "        if (xshift <= center_margin) & (yshift <= center_margin):\n",
    "            roi_x_radius = np.max((roi_x_radius, allradii[idx] + xshift))\n",
    "            roi_y_radius = np.max((roi_y_radius, allradii[idx] + yshift))\n",
    "\n",
    "    if roi_x_radius > 0 and roi_y_radius > 0:\n",
    "        roi_radii = roi_x_radius, roi_y_radius\n",
    "    else:\n",
    "        roi_radii = None\n",
    "\n",
    "    return roi_center, roi_radii"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def imshow(*args,**kwargs):\n",
    "    \"\"\" Handy function to show multiple plots in on row, possibly with different cmaps and titles\n",
    "    Usage: \n",
    "    imshow(img1, title=\"myPlot\")\n",
    "    imshow(img1,img2, title=['title1','title2'])\n",
    "    imshow(img1,img2, cmap='hot')\n",
    "    imshow(img1,img2,cmap=['gray','Blues']) \"\"\"\n",
    "    cmap = kwargs.get('cmap', 'gray')\n",
    "    title= kwargs.get('title','')\n",
    "    if len(args)==0:\n",
    "        raise ValueError(\"No images given to imshow\")\n",
    "    elif len(args)==1:\n",
    "        plt.title(title)\n",
    "        plt.imshow(args[0], interpolation='none')\n",
    "    else:\n",
    "        n=len(args)\n",
    "        if type(cmap)==str:\n",
    "            cmap = [cmap]*n\n",
    "        if type(title)==str:\n",
    "            title= [title]*n\n",
    "        plt.figure(figsize=(n*5,10))\n",
    "        for i in range(n):\n",
    "            plt.subplot(1,n,i+1)\n",
    "            plt.title(title[i])\n",
    "            plt.imshow(args[i], cmap[i])\n",
    "    plt.show()\n",
    "    \n",
    "def plot_roi(data4D, roi_center, roi_radii):\n",
    "    \"\"\"\n",
    "    Do the animation of full heart volume\n",
    "    \"\"\"\n",
    "    x_roi_center, y_roi_center = roi_center[0], roi_center[1]\n",
    "    x_roi_radius, y_roi_radius = roi_radii[0], roi_radii[1]\n",
    "    print 'nslices', data4D.shape[2]\n",
    "\n",
    "    zslices = data4D.shape[2]\n",
    "    tframes = data4D.shape[3]\n",
    "\n",
    "    slice_cnt = 0\n",
    "    for slice in [data4D[:,:,z,:] for z in range(zslices)]:\n",
    "      outdata = np.swapaxes(np.swapaxes(slice[:,:,:], 0,2), 1,2)\n",
    "      roi_mask = np.zeros_like(outdata[0])\n",
    "      roi_mask[x_roi_center - x_roi_radius:x_roi_center + x_roi_radius,\n",
    "      y_roi_center - y_roi_radius:y_roi_center + y_roi_radius] = 1\n",
    "\n",
    "      outdata[:, roi_mask > 0.5] = 0.8 * outdata[:, roi_mask > 0.5]\n",
    "      outdata[:, roi_mask > 0.5] = 0.8 * outdata[:, roi_mask > 0.5]\n",
    "\n",
    "      fig = plt.figure(1)\n",
    "      fig.canvas.set_window_title('slice_No' + str(slice_cnt))\n",
    "      slice_cnt+=1\n",
    "      def init_out():\n",
    "          im.set_data(outdata[0])\n",
    "\n",
    "      def animate_out(i):\n",
    "          im.set_data(outdata[i])\n",
    "          return im\n",
    "\n",
    "      im = fig.gca().imshow(outdata[0], cmap='gray')\n",
    "      anim = animation.FuncAnimation(fig, animate_out, init_func=init_out, frames=tframes, interval=50)\n",
    "      anim.save('Cine_MRI_SAX_%d.mp4'%slice_cnt, fps=50, extra_args=['-vcodec', 'libx264'])\n",
    "      plt.show()\n",
    "        \n",
    "def plot_4D(data4D):\n",
    "    \"\"\"\n",
    "    Do the animation of full heart volume\n",
    "    \"\"\"\n",
    "    print 'nslices', data4D.shape[2]\n",
    "    zslices = data4D.shape[2]\n",
    "    tframes = data4D.shape[3]\n",
    "\n",
    "    slice_cnt = 0\n",
    "    for slice in [data4D[:,:,z,:] for z in range(zslices)]:\n",
    "      outdata = np.swapaxes(np.swapaxes(slice[:,:,:], 0,2), 1,2)\n",
    "      fig = plt.figure(1)\n",
    "      fig.canvas.set_window_title('slice_No' + str(slice_cnt))\n",
    "      slice_cnt+=1\n",
    "      def init_out():\n",
    "          im.set_data(outdata[0])\n",
    "\n",
    "      def animate_out(i):\n",
    "          im.set_data(outdata[i])\n",
    "          return im\n",
    "\n",
    "      im = fig.gca().imshow(outdata[0], cmap='gray')\n",
    "      anim = animation.FuncAnimation(fig, animate_out, init_func=init_out, frames=tframes, interval=50)\n",
    "      plt.show()\n",
    "\n",
    "\n",
    "def multilabel_split(image_tensor):\n",
    "    \"\"\"\n",
    "    image_tensor : Batch * H * W\n",
    "    Split multilabel images and return stack of images\n",
    "    Returns: Tensor of shape: Batch * H * W * n_class (4D tensor)\n",
    "    # TODO: Be careful: when using this code: labels need to be \n",
    "    defined, explictly before hand as this code does not handle\n",
    "    missing labels\n",
    "    So far, this function is okay as it considers full volume for\n",
    "    finding out unique labels\n",
    "    \"\"\"\n",
    "    labels = np.unique(image_tensor)\n",
    "    batch_size = image_tensor.shape[0]\n",
    "    out_shape =  image_tensor.shape + (len(labels),)\n",
    "    image_tensor_4D = np.zeros(out_shape, dtype='uint8')\n",
    "    for i in xrange(batch_size):\n",
    "        cnt = 0\n",
    "        shape =image_tensor.shape[1:3] + (len(labels),)\n",
    "        temp = np.ones(shape, dtype='uint8')\n",
    "        for label in labels:\n",
    "            temp[...,cnt] = np.where(image_tensor[i] == label, temp[...,cnt], 0)\n",
    "            cnt += 1\n",
    "        image_tensor_4D[i] = temp\n",
    "    return image_tensor_4D\n",
    "\n",
    "def save_data(data, filename, out_path):\n",
    "    out_filename = os.path.join(out_path, filename)\n",
    "    with open(out_filename, 'wb') as f:\n",
    "        pickle.dump(data, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    print 'saved to %s' % out_filename\n",
    "\n",
    "def load_pkl(path):\n",
    "    with open(path) as f:\n",
    "        obj = pickle.load(f)\n",
    "    return obj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Heart Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def heart_metrics(seg_3Dmap, voxel_size, classes=[3, 1, 2]):\n",
    "    \"\"\"\n",
    "    Compute the volumes of each classes\n",
    "    \"\"\"\n",
    "    # Loop on each classes of the input images\n",
    "    volumes = []\n",
    "    for c in classes:\n",
    "        # Copy the gt image to not alterate the input\n",
    "        seg_3Dmap_copy = np.copy(seg_3Dmap)\n",
    "        seg_3Dmap_copy[seg_3Dmap_copy != c] = 0\n",
    "\n",
    "        # Clip the value to compute the volumes\n",
    "        seg_3Dmap_copy = np.clip(seg_3Dmap_copy, 0, 1)\n",
    "\n",
    "        # Compute volume\n",
    "        volume = seg_3Dmap_copy.sum() * np.prod(voxel_size) / 1000.\n",
    "        volumes += [volume]\n",
    "    return volumes\n",
    "\n",
    "def ejection_fraction(ed_vol, es_vol):\n",
    "    \"\"\"\n",
    "    Calculate ejection fraction\n",
    "    \"\"\"\n",
    "    stroke_vol = ed_vol - es_vol\n",
    "    return (np.float(stroke_vol)/np.float(ed_vol))*100\n",
    "\n",
    "def myocardialmass(myocardvol):\n",
    "    \"\"\"\n",
    "    Specific gravity of heart muscle (1.05 g/ml)\n",
    "    \"\"\" \n",
    "    return myocardvol*1.05"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stratified Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Refer:\n",
    "# http://www.echopedia.org/wiki/Left_Ventricular_Dimensions\n",
    "# https://www.creatis.insa-lyon.fr/Challenge/acdc/databases.html\n",
    "# https://en.wikipedia.org/wiki/Body_surface_area\n",
    "# 30 normal subjects - NOR\n",
    "NORMAL = 'NOR'\n",
    "# 30 patients with previous myocardial infarction \n",
    "# (ejection fraction of the left ventricle lower than 40% and several myocardial segments with abnormal contraction) - MINF\n",
    "MINF = 'MINF'\n",
    "# 30 patients with dilated cardiomyopathy \n",
    "# (diastolic left ventricular volume >100 mL/m2 and an ejection fraction of the left ventricle lower than 40%) - DCM\n",
    "DCM = 'DCM'\n",
    "# 30 patients with hypertrophic cardiomyopathy \n",
    "# (left ventricular cardiac mass high than 110 g/m2,\n",
    "# several myocardial segments with a thickness higher than 15 mm in diastole and a normal ejecetion fraction) - HCM\n",
    "HCM = 'HCM'\n",
    "# 30 patients with abnormal right ventricle (volume of the right ventricular \n",
    "# cavity higher than 110 mL/m2 or ejection fraction of the rigth ventricle lower than 40%) - RV\n",
    "RV = 'RV'\n",
    "def copy(src, dest):\n",
    "  \"\"\"\n",
    "  Copy function\n",
    "  \"\"\"\n",
    "  try:\n",
    "      shutil.copytree(src, dest, ignore=shutil.ignore_patterns())\n",
    "  except OSError as e:\n",
    "      # If the error was caused because the source wasn't a directory\n",
    "      if e.errno == errno.ENOTDIR:\n",
    "          shutil.copy(src, dest)\n",
    "      else:\n",
    "          print('Directory not copied. Error: %s' % e)\n",
    "\n",
    "def read_patient_cfg(path):\n",
    "  \"\"\"\n",
    "  Reads patient data in the cfg file and returns a dictionary\n",
    "  \"\"\"\n",
    "  patient_info = {}\n",
    "  with open(os.path.join(path, 'Info.cfg')) as f_in:\n",
    "    for line in f_in:\n",
    "      l = line.rstrip().split(\": \")\n",
    "      patient_info[l[0]] = l[1]\n",
    "  return patient_info\n",
    "     \n",
    "def group_patient_cases(src_path, out_path, force=False):\n",
    "  \"\"\" Group the patient data according to cardiac pathology\"\"\" \n",
    "\n",
    "  cases = sorted(next(os.walk(src_path))[1])\n",
    "  dest_path = os.path.join(out_path, 'Patient_Groups')\n",
    "  if force:\n",
    "    shutil.rmtree(dest_path)\n",
    "  if os.path.exists(dest_path):\n",
    "    return dest_path  \n",
    "\n",
    "  os.makedirs(dest_path)\n",
    "  os.mkdir(os.path.join(dest_path, NORMAL))\n",
    "  os.mkdir(os.path.join(dest_path, MINF))\n",
    "  os.mkdir(os.path.join(dest_path, DCM))\n",
    "  os.mkdir(os.path.join(dest_path, HCM))\n",
    "  os.mkdir(os.path.join(dest_path, RV))\n",
    "\n",
    "  for case in cases:\n",
    "    full_path = os.path.join(src_path, case)\n",
    "    copy(full_path, os.path.join(dest_path,\\\n",
    "        read_patient_cfg(full_path)['Group'], case))\n",
    "\n",
    "def generate_train_validate_test_set(src_path, dest_path):\n",
    "  \"\"\"\n",
    "  Split the data into 70:15:15 for train-validate-test set\n",
    "  arg: path: input data path\n",
    "  \"\"\"\n",
    "  SPLIT_TRAIN = 0.7\n",
    "  SPLIT_VALID = 0.15\n",
    "\n",
    "  dest_path = os.path.join(dest_path,'dataset')\n",
    "  if os.path.exists(dest_path):\n",
    "    shutil.rmtree(dest_path)\n",
    "  os.makedirs(os.path.join(dest_path, 'train_set'))  \n",
    "  os.makedirs(os.path.join(dest_path, 'validation_set'))  \n",
    "  os.makedirs(os.path.join(dest_path, 'test_set'))  \n",
    "  # print (src_path)\n",
    "  groups = next(os.walk(src_path))[1]\n",
    "  for group in groups:\n",
    "    group_path = next(os.walk(os.path.join(src_path, group)))[0]\n",
    "    patient_folders = next(os.walk(group_path))[1]\n",
    "    np.random.shuffle(patient_folders)\n",
    "    train_ = patient_folders[0:int(SPLIT_TRAIN*len(patient_folders))]\n",
    "    valid_ = patient_folders[int(SPLIT_TRAIN*len(patient_folders)): \n",
    "                 int((SPLIT_TRAIN+SPLIT_VALID)*len(patient_folders))]\n",
    "    test_ = patient_folders[int((SPLIT_TRAIN+SPLIT_VALID)*len(patient_folders)):]\n",
    "    for patient in train_:\n",
    "      folder_path = os.path.join(group_path, patient)\n",
    "      copy(folder_path, os.path.join(dest_path, 'train_set', patient))\n",
    "\n",
    "    for patient in valid_:\n",
    "      folder_path = os.path.join(group_path, patient)\n",
    "      copy(folder_path, os.path.join(dest_path, 'validation_set', patient))\n",
    "\n",
    "    for patient in test_:\n",
    "      folder_path = os.path.join(group_path, patient)\n",
    "      copy(folder_path, os.path.join(dest_path, 'test_set', patient))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# complete_data_path = '/home/bmi/Documents/mak/Cardiac_dataset/ACDC/dataset/training'\n",
    "# dest_path = '../../processed_acdc_dataset'\n",
    "# group_path = '../../processed_acdc_dataset/Patient_Groups'\n",
    "# group_patient_cases(complete_data_path, dest_path)\n",
    "# generate_train_validate_test_set(group_path, dest_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data set Reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Dataset(object):\n",
    "    def __init__(self, directory, subdir):\n",
    "        # type: (object, object) -> object\n",
    "        self.patient_data = {}\n",
    "        self.directory = directory\n",
    "        self.name = subdir\n",
    "\n",
    "    def _filename(self, file):\n",
    "        return os.path.join(self.directory, self.name, file)\n",
    "\n",
    "    def load_nii(self, img_path):\n",
    "        \"\"\"\n",
    "        Function to load a 'nii' or 'nii.gz' file, The function returns\n",
    "        everyting needed to save another 'nii' or 'nii.gz'\n",
    "        in the same dimensional space, i.e. the affine matrix and the header\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "\n",
    "        img_path: string\n",
    "        String with the path of the 'nii' or 'nii.gz' image file name.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Three element, the first is a numpy array of the image values,\n",
    "        the second is the affine transformation of the image, and the\n",
    "        last one is the header of the image.\n",
    "        \"\"\"\n",
    "        nimg = nib.load(self._filename(img_path))\n",
    "        return nimg.get_data(), nimg.affine, nimg.header\n",
    "    \n",
    "    def read_patient_info_data(self):\n",
    "        \"\"\"\n",
    "        Reads patient data in the cfg file from patient folder \n",
    "        using Info.cfg\n",
    "        \"\"\"\n",
    "        with open(self._filename('Info.cfg')) as f_in:\n",
    "            for line in f_in:\n",
    "              l = line.rstrip().split(\": \")\n",
    "              self.patient_data[l[0]] = l[1]\n",
    "\n",
    "    def read_patient_data(self, mode='train', roi_detect=True):\n",
    "        \"\"\"\n",
    "        Reads patient data in the cfg file and returns a dictionary and\n",
    "        extract End diastole and End Systole image from patient folder\n",
    "        using Info.cfg\n",
    "        \"\"\"\n",
    "        self.read_patient_info_data()\n",
    "        # Read patient Number\n",
    "        m = re.match(\"patient(\\d{3})\", self.name)\n",
    "        patient_No = int(m.group(1))\n",
    "        # Read Diastole frame Number\n",
    "        ED_frame_No = int(self.patient_data['ED'])\n",
    "        ed_img = \"patient%03d_frame%02d.nii.gz\" %(patient_No, ED_frame_No)\n",
    "        ed, affine, hdr  = self.load_nii(ed_img)\n",
    "        # Read Systole frame Number\n",
    "        ES_frame_No = int(self.patient_data['ES'])\n",
    "        es_img = \"patient%03d_frame%02d.nii.gz\" %(patient_No, ES_frame_No)\n",
    "        es, _, _  = self.load_nii(es_img)\n",
    "        # Save Images:\n",
    "        self.patient_data['ED_VOL'] = ed\n",
    "        self.patient_data['ES_VOL'] = es\n",
    " \n",
    "        # Header Info for saving    \n",
    "        header_info ={'affine':affine, 'hdr': hdr}\n",
    "        self.patient_data['header'] = header_info\n",
    "        if mode == 'train':\n",
    "            ed_gt, _, _  = self.load_nii(\"patient%03d_frame%02d_gt.nii.gz\" %(patient_No, ED_frame_No))\n",
    "            es_gt, _, _  = self.load_nii(\"patient%03d_frame%02d_gt.nii.gz\" %(patient_No, ES_frame_No))\n",
    "            ed_lv, ed_rv, ed_myo = heart_metrics(ed_gt, hdr.get_zooms()) \n",
    "            es_lv, es_rv, es_myo = heart_metrics(es_gt, hdr.get_zooms())\n",
    "            ef_lv = ejection_fraction(ed_lv, es_lv)\n",
    "            ef_rv = ejection_fraction(ed_rv, es_rv)\n",
    "            heart_param = {'EDV_LV': ed_lv, 'EDV_RV': ed_rv, 'ESV_LV': es_lv, 'ESV_RV': es_rv,\n",
    "                           'ED_MYO': ed_myo, 'ES_MYO': es_myo, 'EF_LV': ef_lv, 'EF_RV': ef_rv}  \n",
    "            self.patient_data['HP'] = heart_param \n",
    "            self.patient_data['ED_GT'] = ed_gt\n",
    "            self.patient_data['ES_GT'] = es_gt\n",
    "        if roi_detect:\n",
    "            # Read a particular volume number in 4D image\n",
    "            img_4d = \"patient%03d_4d.nii.gz\"%patient_No\n",
    "            # Load data\n",
    "            img_4D, _, hdr = self.load_nii(img_4d)\n",
    "            c, r = extract_roi(img_4D, hdr.get_zooms()) \n",
    "            self.patient_data['roi_center'], self.patient_data['roi_radii']=c,r \n",
    "#             print c, r\n",
    "#             plot_roi(img_4D, c,r)\n",
    "            \n",
    "def convert_nii_np(data_path, mode, roi_detect):\n",
    "    \"\"\"\n",
    "    Prepare a dictionary of dataset and save it as numpy file\n",
    "    \"\"\"\n",
    "    patient_fulldata = OrderedDict()\n",
    "    print data_path\n",
    "    patient_folders = next(os.walk(data_path))[1]\n",
    "    for patient in sorted(patient_folders):\n",
    "#         print (patient)\n",
    "        dset = Dataset(data_path, patient)\n",
    "        dset.read_patient_data(mode=mode, roi_detect=roi_detect)\n",
    "        patient_fulldata[dset.name] = dset.patient_data\n",
    "    return patient_fulldata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "# Train Data\n",
    "complete_train_path = '/home/bmi/Documents/mak/Cardiac_dataset/ACDC/dataset/training'\n",
    "train_dataset = '../../processed_acdc_dataset/dataset/train_set'\n",
    "validation_dataset = '../../processed_acdc_dataset/dataset/validation_set'\n",
    "test_dataset = '../../processed_acdc_dataset/dataset/test_set'\n",
    "\n",
    "final_testing_dataset = '/home/bmi/Documents/mak/Cardiac_dataset/ACDC/dataset/testing'\n",
    "out_path_train = '../../processed_acdc_dataset/pickled/full_data'\n",
    "out_path_test = '../../processed_acdc_dataset/pickled/final_test'\n",
    "\n",
    "\n",
    "\n",
    "print ('ROI->ED->ES train dataset')\n",
    "if not os.path.exists(out_path_train):\n",
    "    # shutil.rmtree(out_path_train)\n",
    "    os.makedirs(out_path_train)\n",
    "    os.makedirs(out_path_test)\n",
    "    \n",
    "train_dataset = convert_nii_np(train_dataset, mode='train', roi_detect=True)\n",
    "save_data(training_dataset, 'train_data_roi.pkl', out_path_train)\n",
    "print(\"---Processing Training dataset %s seconds ---\" % (time.time() - start_time))\n",
    "validation_dataset = convert_nii_np(validation_dataset, mode='train', roi_detect=True)\n",
    "save_data(validation_dataset, 'validation_data_roi.pkl', out_path_train)\n",
    "print(\"---Processing Training dataset %s seconds ---\" % (time.time() - start_time))\n",
    "local_test_dataset = convert_nii_np(local_test_dataset, mode='train', roi_detect=True)\n",
    "save_data(local_test_dataset, 'test_data_roi.pkl', out_path_train)\n",
    "print(\"---Processing Training dataset %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "start_time = time.time()\n",
    "print ('ROI->ED->ES test dataset')\n",
    "final_test_dataset = convert_nii_np(final_testing_dataset, mode='test', roi_detect=True)\n",
    "save_data(final_test_dataset, 'final_testing_data.pkl', out_path_test)\n",
    "print(\"---Processing Training dataset %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "out_path = '../../processed_acdc_dataset/hdf5_files'\n",
    "mode = 'full_data'\n",
    "if not os.path.exists(os.path.join(out_path, mode)):\n",
    "    os.makedirs(os.path.join(out_path, mode))\n",
    "for patient_id in full_train_dataset.keys():\n",
    "#     print (patient_id)\n",
    "    _id = patient_id[-3:]\n",
    "    hp = h5py.File(os.path.join(out_path_train, mode, 'patient'+'_'+_id+'.hdf5'),'w')\n",
    "    hp.create_dataset('ED', data=full_train_dataset[patient_id]['ED_VOL'])\n",
    "    hp.create_dataset('ED_GT', data=full_train_dataset[patient_id]['ED_GT'])\n",
    "    hp.create_dataset('ES', data=full_train_dataset[patient_id]['ED_VOL'])\n",
    "    hp.create_dataset('ES_GT', data=full_train_dataset[patient_id]['ED_GT'])\n",
    "    hp.create_dataset('GROUP', data=full_train_dataset[patient_id]['Group'])\n",
    "    hp.close()\n",
    "\n",
    "mode = 'final_test'\n",
    "if not os.path.exists(os.path.join(out_path, mode)):\n",
    "    os.makedirs(os.path.join(out_path, mode))\n",
    "for patient_id in final_test_dataset.keys():\n",
    "#     print (patient_id)\n",
    "    _id = patient_id[-3:]\n",
    "    hp = h5py.File(os.path.join(out_path_train, mode, 'patient'+'_'+_id+'.hdf5'),'w')\n",
    "    hp.create_dataset('ED', data=full_train_dataset[patient_id]['ED_VOL'])\n",
    "    hp.create_dataset('ES', data=full_train_dataset[patient_id]['ED_VOL'])\n",
    "    hp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "out_path = '../../dataset/hdf5_files'\n",
    "mode = 'full_data'\n",
    "data_path = os.path.join(out_path, mode)\n",
    "patient_files = next(os.walk(data_path))[2]\n",
    "for patient in sorted(patient_files):\n",
    "    print patient\n",
    "    h5 = h5py.File(os.path.join(data_path,patient),'r')\n",
    "    print h5\n",
    "    print h5['ED'][:,:,5].dtype\n",
    "    print h5['ED_GT'][:,:,5].dtype\n",
    "    imshow(h5['ED'][:,:,5], h5['ED_GT'][:,:,5])\n",
    "    imshow(h5['ES'][:,:,5], h5['ES_GT'][:,:,5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
